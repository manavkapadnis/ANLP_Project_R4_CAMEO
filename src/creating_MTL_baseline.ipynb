{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import LightningDataModule\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoImageProcessor\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,AutoImageProcessor, AutoModel\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from evalcap.bleu.bleu import Bleu\n",
    "from evalcap.rouge.rouge import Rouge\n",
    "from evalcap.cider.cider import Cider\n",
    "from evalcap.meteor.meteor import Meteor\n",
    "from transformers import SwinModel\n",
    "from lightning_tools.optim import config_optimizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import pdb\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMG</th>\n",
       "      <th>QSN</th>\n",
       "      <th>TXT</th>\n",
       "      <th>OBJ</th>\n",
       "      <th>COL</th>\n",
       "      <th>CNT</th>\n",
       "      <th>OTH</th>\n",
       "      <th>captions</th>\n",
       "      <th>vqa_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VizWiz_train_00000000.jpg</td>\n",
       "      <td>What's the name of this product?</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS...</td>\n",
       "      <td>['basil leaves', 'basil']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VizWiz_train_00000001.jpg</td>\n",
       "      <td>Can you tell me what is in this can please?</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['A can of Coca Cola on a counter is shown for...</td>\n",
       "      <td>['coca cola 0', 'coke 0', 'coke', 'unanswerabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VizWiz_train_00000002.jpg</td>\n",
       "      <td>Is this enchilada sauce or is this tomatoes?  ...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['A can of crushed tomatoes are on a brown sur...</td>\n",
       "      <td>['these tomatoes not enchilada sauce', 'crushe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VizWiz_train_00000003.jpg</td>\n",
       "      <td>What is the captcha on this screenshot?</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['A white screen with a captcha that needs to ...</td>\n",
       "      <td>['t36m', 't 3 6 m', 't63m']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VizWiz_train_00000004.jpg</td>\n",
       "      <td>What is this item?</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"A box for a garden light rests in someone's ...</td>\n",
       "      <td>['solar garden light', 'solar light', 'garden ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         IMG  \\\n",
       "0  VizWiz_train_00000000.jpg   \n",
       "1  VizWiz_train_00000001.jpg   \n",
       "2  VizWiz_train_00000002.jpg   \n",
       "3  VizWiz_train_00000003.jpg   \n",
       "4  VizWiz_train_00000004.jpg   \n",
       "\n",
       "                                                 QSN  TXT  OBJ  COL  CNT  OTH  \\\n",
       "0                   What's the name of this product?    4    3    0    0    0   \n",
       "1        Can you tell me what is in this can please?    2    5    1    1    0   \n",
       "2  Is this enchilada sauce or is this tomatoes?  ...    4    3    1    0    0   \n",
       "3            What is the captcha on this screenshot?    4    2    1    1    0   \n",
       "4                                 What is this item?    3    5    1    0    0   \n",
       "\n",
       "                                            captions  \\\n",
       "0  ['ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS...   \n",
       "1  ['A can of Coca Cola on a counter is shown for...   \n",
       "2  ['A can of crushed tomatoes are on a brown sur...   \n",
       "3  ['A white screen with a captcha that needs to ...   \n",
       "4  [\"A box for a garden light rests in someone's ...   \n",
       "\n",
       "                                         vqa_answers  \n",
       "0                          ['basil leaves', 'basil']  \n",
       "1  ['coca cola 0', 'coke 0', 'coke', 'unanswerabl...  \n",
       "2  ['these tomatoes not enchilada sauce', 'crushe...  \n",
       "3                        ['t36m', 't 3 6 m', 't63m']  \n",
       "4  ['solar garden light', 'solar light', 'garden ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/mkapadni/work/anlp_project/data/MTL_train_new.csv\")\n",
    "df_val = pd.read_csv(\"/home/mkapadni/work/anlp_project/data/MTL_val.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMG</th>\n",
       "      <th>QSN</th>\n",
       "      <th>TXT</th>\n",
       "      <th>OBJ</th>\n",
       "      <th>COL</th>\n",
       "      <th>CNT</th>\n",
       "      <th>captions</th>\n",
       "      <th>vqa_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VizWiz_train_00000000.jpg</td>\n",
       "      <td>What's the name of this product?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS...</td>\n",
       "      <td>['basil leaves', 'basil']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VizWiz_train_00000001.jpg</td>\n",
       "      <td>Can you tell me what is in this can please?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['A can of Coca Cola on a counter is shown for...</td>\n",
       "      <td>['coca cola 0', 'coke 0', 'coke', 'unanswerabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VizWiz_train_00000002.jpg</td>\n",
       "      <td>Is this enchilada sauce or is this tomatoes?  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['A can of crushed tomatoes are on a brown sur...</td>\n",
       "      <td>['these tomatoes not enchilada sauce', 'crushe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VizWiz_train_00000003.jpg</td>\n",
       "      <td>What is the captcha on this screenshot?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['A white screen with a captcha that needs to ...</td>\n",
       "      <td>['t36m', 't 3 6 m', 't63m']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VizWiz_train_00000004.jpg</td>\n",
       "      <td>What is this item?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"A box for a garden light rests in someone's ...</td>\n",
       "      <td>['solar garden light', 'solar light', 'garden ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         IMG  \\\n",
       "0  VizWiz_train_00000000.jpg   \n",
       "1  VizWiz_train_00000001.jpg   \n",
       "2  VizWiz_train_00000002.jpg   \n",
       "3  VizWiz_train_00000003.jpg   \n",
       "4  VizWiz_train_00000004.jpg   \n",
       "\n",
       "                                                 QSN  TXT  OBJ  COL  CNT  \\\n",
       "0                   What's the name of this product?    1    0    0    0   \n",
       "1        Can you tell me what is in this can please?    0    1    0    0   \n",
       "2  Is this enchilada sauce or is this tomatoes?  ...    1    0    0    0   \n",
       "3            What is the captcha on this screenshot?    1    0    0    0   \n",
       "4                                 What is this item?    0    1    0    0   \n",
       "\n",
       "                                            captions  \\\n",
       "0  ['ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS...   \n",
       "1  ['A can of Coca Cola on a counter is shown for...   \n",
       "2  ['A can of crushed tomatoes are on a brown sur...   \n",
       "3  ['A white screen with a captcha that needs to ...   \n",
       "4  [\"A box for a garden light rests in someone's ...   \n",
       "\n",
       "                                         vqa_answers  \n",
       "0                          ['basil leaves', 'basil']  \n",
       "1  ['coca cola 0', 'coke 0', 'coke', 'unanswerabl...  \n",
       "2  ['these tomatoes not enchilada sauce', 'crushe...  \n",
       "3                        ['t36m', 't 3 6 m', 't63m']  \n",
       "4  ['solar garden light', 'solar light', 'garden ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the TXT, OBJ, CNT, COL columns as if value more than 3, then 1 else 0 and drop OTH column\n",
    "df['TXT'] = df['TXT'].apply(lambda x: 1 if x > 3 else 0)\n",
    "df['OBJ'] = df['OBJ'].apply(lambda x: 1 if x > 3 else 0)\n",
    "df['CNT'] = df['CNT'].apply(lambda x: 1 if x > 3 else 0)\n",
    "df['COL'] = df['COL'].apply(lambda x: 1 if x > 3 else 0)\n",
    "df.drop(['OTH'], axis=1, inplace=True)\n",
    "df_val['TXT'] = df_val['TXT'].apply(lambda x: 1 if x > 3 else 0)\n",
    "df_val['OBJ'] = df_val['OBJ'].apply(lambda x: 1 if x > 3 else 0)\n",
    "df_val['CNT'] = df_val['CNT'].apply(lambda x: 1 if x > 3 else 0)\n",
    "df_val['COL'] = df_val['COL'].apply(lambda x: 1 if x > 3 else 0)\n",
    "df_val.drop(['OTH'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMG</th>\n",
       "      <th>QSN</th>\n",
       "      <th>TXT</th>\n",
       "      <th>OBJ</th>\n",
       "      <th>COL</th>\n",
       "      <th>CNT</th>\n",
       "      <th>captions</th>\n",
       "      <th>vqa_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VizWiz_train_00000000.jpg</td>\n",
       "      <td>What's the name of this product?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS ...</td>\n",
       "      <td>[basil leaves, basil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VizWiz_train_00000001.jpg</td>\n",
       "      <td>Can you tell me what is in this can please?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[A can of Coca Cola on a counter is shown for ...</td>\n",
       "      <td>[coca cola 0, coke 0, coke, unanswerable, soda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VizWiz_train_00000002.jpg</td>\n",
       "      <td>Is this enchilada sauce or is this tomatoes?  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[A can of crushed tomatoes are on a brown surf...</td>\n",
       "      <td>[these tomatoes not enchilada sauce, crushed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VizWiz_train_00000003.jpg</td>\n",
       "      <td>What is the captcha on this screenshot?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[A white screen with a captcha that needs to b...</td>\n",
       "      <td>[t36m, t 3 6 m, t63m]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VizWiz_train_00000004.jpg</td>\n",
       "      <td>What is this item?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[A box for a garden light rests in someone's l...</td>\n",
       "      <td>[solar garden light, solar light, garden light]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         IMG  \\\n",
       "0  VizWiz_train_00000000.jpg   \n",
       "1  VizWiz_train_00000001.jpg   \n",
       "2  VizWiz_train_00000002.jpg   \n",
       "3  VizWiz_train_00000003.jpg   \n",
       "4  VizWiz_train_00000004.jpg   \n",
       "\n",
       "                                                 QSN  TXT  OBJ  COL  CNT  \\\n",
       "0                   What's the name of this product?    1    0    0    0   \n",
       "1        Can you tell me what is in this can please?    0    1    0    0   \n",
       "2  Is this enchilada sauce or is this tomatoes?  ...    1    0    0    0   \n",
       "3            What is the captcha on this screenshot?    1    0    0    0   \n",
       "4                                 What is this item?    0    1    0    0   \n",
       "\n",
       "                                            captions  \\\n",
       "0  [ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS ...   \n",
       "1  [A can of Coca Cola on a counter is shown for ...   \n",
       "2  [A can of crushed tomatoes are on a brown surf...   \n",
       "3  [A white screen with a captcha that needs to b...   \n",
       "4  [A box for a garden light rests in someone's l...   \n",
       "\n",
       "                                         vqa_answers  \n",
       "0                              [basil leaves, basil]  \n",
       "1  [coca cola 0, coke 0, coke, unanswerable, soda...  \n",
       "2  [these tomatoes not enchilada sauce, crushed t...  \n",
       "3                              [t36m, t 3 6 m, t63m]  \n",
       "4    [solar garden light, solar light, garden light]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['captions'] = df['captions'].apply(literal_eval)\n",
    "df['vqa_answers'] = df['vqa_answers'].apply(literal_eval)\n",
    "df_val['captions'] = df_val['captions'].apply(literal_eval)\n",
    "df_val['vqa_answers'] = df_val['vqa_answers'].apply(literal_eval)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMG</th>\n",
       "      <th>QSN</th>\n",
       "      <th>TXT</th>\n",
       "      <th>OBJ</th>\n",
       "      <th>COL</th>\n",
       "      <th>CNT</th>\n",
       "      <th>captions</th>\n",
       "      <th>vqa_answers</th>\n",
       "      <th>answer_counts</th>\n",
       "      <th>highest_confidence_caption</th>\n",
       "      <th>highest_confidence_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VizWiz_train_00000000.jpg</td>\n",
       "      <td>What's the name of this product?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS ...</td>\n",
       "      <td>[basil leaves, basil]</td>\n",
       "      <td>2</td>\n",
       "      <td>ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS T...</td>\n",
       "      <td>basil leaves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VizWiz_train_00000001.jpg</td>\n",
       "      <td>Can you tell me what is in this can please?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[A can of Coca Cola on a counter is shown for ...</td>\n",
       "      <td>[coca cola 0, coke 0, coke, unanswerable, soda...</td>\n",
       "      <td>6</td>\n",
       "      <td>A can of Coca Cola on a counter is shown for w...</td>\n",
       "      <td>coca cola 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VizWiz_train_00000002.jpg</td>\n",
       "      <td>Is this enchilada sauce or is this tomatoes?  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[A can of crushed tomatoes are on a brown surf...</td>\n",
       "      <td>[these tomatoes not enchilada sauce, crushed t...</td>\n",
       "      <td>3</td>\n",
       "      <td>A can of crushed tomatoes are on a brown surfa...</td>\n",
       "      <td>these tomatoes not enchilada sauce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VizWiz_train_00000003.jpg</td>\n",
       "      <td>What is the captcha on this screenshot?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[A white screen with a captcha that needs to b...</td>\n",
       "      <td>[t36m, t 3 6 m, t63m]</td>\n",
       "      <td>3</td>\n",
       "      <td>A white screen with a captcha that needs to be...</td>\n",
       "      <td>t36m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VizWiz_train_00000004.jpg</td>\n",
       "      <td>What is this item?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[A box for a garden light rests in someone's l...</td>\n",
       "      <td>[solar garden light, solar light, garden light]</td>\n",
       "      <td>3</td>\n",
       "      <td>A box for a garden light rests in someone's lap.</td>\n",
       "      <td>solar garden light</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         IMG  \\\n",
       "0  VizWiz_train_00000000.jpg   \n",
       "1  VizWiz_train_00000001.jpg   \n",
       "2  VizWiz_train_00000002.jpg   \n",
       "3  VizWiz_train_00000003.jpg   \n",
       "4  VizWiz_train_00000004.jpg   \n",
       "\n",
       "                                                 QSN  TXT  OBJ  COL  CNT  \\\n",
       "0                   What's the name of this product?    1    0    0    0   \n",
       "1        Can you tell me what is in this can please?    0    1    0    0   \n",
       "2  Is this enchilada sauce or is this tomatoes?  ...    1    0    0    0   \n",
       "3            What is the captcha on this screenshot?    1    0    0    0   \n",
       "4                                 What is this item?    0    1    0    0   \n",
       "\n",
       "                                            captions  \\\n",
       "0  [ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS ...   \n",
       "1  [A can of Coca Cola on a counter is shown for ...   \n",
       "2  [A can of crushed tomatoes are on a brown surf...   \n",
       "3  [A white screen with a captcha that needs to b...   \n",
       "4  [A box for a garden light rests in someone's l...   \n",
       "\n",
       "                                         vqa_answers  answer_counts  \\\n",
       "0                              [basil leaves, basil]              2   \n",
       "1  [coca cola 0, coke 0, coke, unanswerable, soda...              6   \n",
       "2  [these tomatoes not enchilada sauce, crushed t...              3   \n",
       "3                              [t36m, t 3 6 m, t63m]              3   \n",
       "4    [solar garden light, solar light, garden light]              3   \n",
       "\n",
       "                          highest_confidence_caption  \\\n",
       "0  ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS T...   \n",
       "1  A can of Coca Cola on a counter is shown for w...   \n",
       "2  A can of crushed tomatoes are on a brown surfa...   \n",
       "3  A white screen with a captcha that needs to be...   \n",
       "4   A box for a garden light rests in someone's lap.   \n",
       "\n",
       "            highest_confidence_answer  \n",
       "0                        basil leaves  \n",
       "1                         coca cola 0  \n",
       "2  these tomatoes not enchilada sauce  \n",
       "3                                t36m  \n",
       "4                  solar garden light  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['highest_confidence_caption'] = df['captions'].apply(lambda x: x[0])\n",
    "df['highest_confidence_answer'] = df['vqa_answers'].apply(lambda x: x[0])\n",
    "df_val['highest_confidence_caption'] = df_val['captions'].apply(lambda x: x[0])\n",
    "df_val['highest_confidence_answer'] = df_val['vqa_answers'].apply(lambda x: x[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataloader for train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_processor = AutoImageProcessor.from_pretrained(\"apple/aimv2-large-patch14-224\")\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "\n",
    "# Function to process images\n",
    "def process_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    inputs = vision_processor(images=image, return_tensors=\"pt\")\n",
    "    return inputs['pixel_values']\n",
    "\n",
    "# Function to process text\n",
    "def process_text(text):\n",
    "    inputs = text_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return inputs['input_ids'], inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        # self.vision_processor = AutoImageProcessor.from_pretrained(\"apple/aimv2-large-patch14-224\")\n",
    "        # self.text_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Process image\n",
    "        image_path = os.path.join(self.image_dir, row['IMG'])\n",
    "        image = Image.open(image_path)\n",
    "        # image_features = self.vision_processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "        \n",
    "        # Process text (question)\n",
    "        # question_input_ids, question_attention_mask = self._process_text(row['QSN'])\n",
    "        question = row['QSN']\n",
    "        \n",
    "        # Extract other features from the dataframe\n",
    "        txt_feature = row['TXT']\n",
    "        obj_feature = row['OBJ']\n",
    "        col_feature = row['COL']\n",
    "        cnt_feature = row['CNT']\n",
    "        highest_confidence_caption = row['highest_confidence_caption']\n",
    "        highest_confidence_answer = row['highest_confidence_answer']\n",
    "        # give final string like this -> Caption: A basket of red apples with a price tag reading $5.<SEP>ObjRec:1<SEP>TextRec:1<SEP>ColorRec:1<SEP>Count:1<SEP>Answer:Red<END>\n",
    "        final_string = f\"Caption: {highest_confidence_caption}<SEP>ObjRec:{obj_feature}<SEP>TextRec:{txt_feature}<SEP>ColorRec:{col_feature}<SEP>Count:{cnt_feature}<SEP>Answer:{highest_confidence_answer}<END>\"\n",
    "        \n",
    "        return {\n",
    "            'image_id': row['IMG'],\n",
    "            'image': image,\n",
    "            'question': question,\n",
    "            # 'question_attention_mask': question_attention_mask,\n",
    "            # 'txt': txt_feature,\n",
    "            # 'obj': obj_feature,\n",
    "            # 'col': col_feature,\n",
    "            # 'cnt': cnt_feature,\n",
    "            # 'caption': highest_confidence_caption,\n",
    "            'answer': final_string\n",
    "        }\n",
    "\n",
    "    # def _process_text(self, text):\n",
    "    #     inputs = self.text_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "    #     return inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(args):\n",
    "    train_dataset = VQADataset(df, args.image_dir)\n",
    "    val_dataset = VQADataset(df_val, args.image_dir)\n",
    "    # test_dataset = VQADataset(pd.read_csv(args.test_csv), args.image_dir)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_csv, val_csv, test_csv, image_dir):\n",
    "        super().__init__()\n",
    "        self.train_csv = train_csv\n",
    "        self.val_csv = val_csv\n",
    "        self.test_csv = test_csv\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Load datasets for each stage if necessary\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = VQADataset(self.train_csv, self.image_dir + \"/train\")\n",
    "            self.val_dataset = VQADataset(pd.read_csv(self.val_csv), self.image_dir+\"/val\")\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = VQADataset(pd.read_csv(self.test_csv), self.image_dir)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=16,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=16,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=16,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =\"\"\"You are a helpful vision assistant. Given an image, perform the following tasks:\n",
    "1. **Generate a detailed caption** describing the image contents, including objects, actions, colors, text (if any), and other relevant details.\n",
    "\n",
    "2. **Determine if the following skills are required** to answer each question below. For each question, output a binary value (`1` for Yes, `0` for No) for the following skills:\n",
    "   - **Object Recognition is needed to answer the question (ObjRec)**  \n",
    "   - **Text Recognition is needed to answer the question (TextRec)**  \n",
    "   - **Color Recognition is needed to answer the question (ColorRec)**  \n",
    "   - **Counting is needed to answer the question (Count)**  \n",
    "\n",
    "3. **Answer the question** directly in 1-3 words based on the image and caption.\n",
    "\n",
    "### Format your output:\n",
    "Generate a single string output with the following structure:\n",
    "Caption:{Generated Caption}<SEP>ObjRec: {1 or 0}<SEP>TextRec: {1 or 0}<SEP>ColorRec: {1 or 0}<SEP>Count: {1 or 0}<SEP>Answer: {Direct Answer in 1-3 Words}\n",
    "\n",
    "### Example Input:\n",
    "Question: What is the color of the apples?\n",
    "Image of a group of red apples in a basket with a price tag reading \"$5\".\n",
    "  \n",
    "\n",
    "### Example Output:\n",
    "Caption: A basket of red apples with a price tag reading $5.<SEP>ObjRec:1<SEP>TextRec:1<SEP>ColorRec:1<SEP>Count:1<SEP>Answer:Red<END>\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskVQAModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    MultiTaskVQAModel model.\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.save_hyperparameters(args)\n",
    "\n",
    "        print(f'Loading vision encoder:{args.vision_model}')\n",
    "        self.visual_encoder = AutoModel.from_pretrained.from_pretrained(args.vision_model)\n",
    "        if args.vis_use_lora:\n",
    "            peft_config_visual = LoraConfig(\n",
    "                                    r=args.vis_r,\n",
    "                                    lora_alpha=args.vis_alpha,\n",
    "                                    target_modules=[\"query\", \"value\"],\n",
    "                                    lora_dropout=args.lora_dropout,\n",
    "                                    bias=\"none\",\n",
    "                                    modules_to_save=[\"classifier\"],\n",
    "                                )\n",
    "            self.visual_encoder = get_peft_model(self.visual_encoder, peft_config_visual)\n",
    "            self.visual_encoder.print_trainable_parameters()\n",
    "            print('Loading vision encoder with LoRA -- Done')\n",
    "        elif args.freeze_vm:\n",
    "            for name, param in self.visual_encoder.named_parameters():\n",
    "                param.requires_grad = False\n",
    "            print(f'Loading Frozen vision encoder:{args.vision_model} -- Done')\n",
    "        else:\n",
    "            print(f'Loading Trainable vision encoder:{args.vision_model} -- Done')\n",
    "\n",
    "        print('Loading LLAMA')\n",
    "        self.llama_tokenizer = AutoTokenizer.from_pretrained(args.llama_model, use_fast=False)\n",
    "        self.llama_tokenizer.pad_token_id = 0\n",
    "        if args.low_resource:\n",
    "            self.llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "                args.llama_model,\n",
    "                torch_dtype=\"auto\",\n",
    "                # load_in_8bit=True,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            self.llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "                args.llama_model,\n",
    "                torch_dtype=\"auto\",\n",
    "            )\n",
    "         \n",
    "        if args.llm_use_lora:\n",
    "            self.embed_tokens = self.llama_model.get_input_embeddings()\n",
    "            peft_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM, inference_mode=False, r=args.llm_r, lora_alpha=args.llm_alpha, lora_dropout=args.lora_dropout\n",
    "            )\n",
    "            self.llama_model = get_peft_model(self.llama_model, peft_config)\n",
    "            self.llama_model.print_trainable_parameters()\n",
    "            print('Loading LLM LoRA Done')         \n",
    "        else:\n",
    "            self.embed_tokens = self.llama_model.get_input_embeddings()\n",
    "            for name, param in self.llama_model.named_parameters():\n",
    "                param.requires_grad = False\n",
    "            print('Loading LLM Done')\n",
    "\n",
    "        self.llama_proj = nn.Linear(self.visual_encoder.num_features, self.llama_model.config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(self.llama_model.config.hidden_size)\n",
    "        self.end_sym = args.end_sym\n",
    "        self.prompt = prompt\n",
    "        self.val_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "        self.val_score = 0.0\n",
    "\n",
    "        if args.delta_file is not None:\n",
    "            state_dict = torch.load(args.delta_file, map_location=torch.device(f'cuda:{torch.cuda.current_device()}'))['model']\n",
    "            self.load_state_dict(state_dict=state_dict, strict=False)\n",
    "            print(f'Load checkpoint from {args.delta_file}')\n",
    "\n",
    "\n",
    "    def score(self, ref, hypo):\n",
    "        \"\"\"\n",
    "        ref, dictionary of reference sentences (id, sentence)\n",
    "        hypo, dictionary of hypothesis sentences (id, sentence)\n",
    "        score, dictionary of scores\n",
    "        \"\"\"\n",
    "        scorers = [\n",
    "            (Bleu(4), [\"Bleu_1\", \"Bleu_4\"]),\n",
    "            (Rouge(), \"ROUGE_L\"),\n",
    "            # (Meteor(), \"METEOR\"),\n",
    "            (Cider(), \"CIDEr\")\n",
    "        ]\n",
    "        final_scores = {}\n",
    "        for scorer, method in scorers:\n",
    "            score, scores = scorer.compute_score(ref, hypo)\n",
    "            if type(score) == list:\n",
    "                for m, s in zip(method, score):\n",
    "                    final_scores[m] = s\n",
    "            else:\n",
    "                final_scores[method] = score\n",
    "        return final_scores\n",
    "\n",
    "\n",
    "    def encode_img(self, images):\n",
    "        image_embeds = []\n",
    "        for image in images:\n",
    "            device = image.device\n",
    "            if self.hparams.global_only:\n",
    "                image_embed = self.visual_encoder(image)['pooler_output'].unsqueeze(1).to(device)\n",
    "            else:\n",
    "                image_embed = self.visual_encoder(image)['last_hidden_state'].to(device)\n",
    "            image_embeds.append(image_embed)\n",
    "            \n",
    "        image_embeds = torch.stack(image_embeds).mean(0)\n",
    "        inputs_llama = self.llama_proj(image_embeds)\n",
    "        atts_llama = torch.ones(inputs_llama.size()[:-1], dtype=torch.long).to(image.device)\n",
    "        return inputs_llama, atts_llama\n",
    "\n",
    "\n",
    "    def prompt_wrap(self, img_embeds, atts_img):\n",
    "        prompt=f'Human: <Img><ImageHere></Img> {self.prompt} \\nAssistant:'\n",
    "        batch_size = img_embeds.shape[0]\n",
    "        p_before, p_after = prompt.split('<ImageHere>')\n",
    "        p_before_tokens = self.llama_tokenizer(\n",
    "            p_before, return_tensors=\"pt\", add_special_tokens=False).to(img_embeds.device)\n",
    "        p_after_tokens = self.llama_tokenizer(\n",
    "            p_after, return_tensors=\"pt\", add_special_tokens=False).to(img_embeds.device)\n",
    "        p_before_embeds = self.embed_tokens(p_before_tokens.input_ids).expand(batch_size, -1, -1)\n",
    "        p_after_embeds = self.embed_tokens(p_after_tokens.input_ids).expand(batch_size, -1, -1)\n",
    "        wrapped_img_embeds = torch.cat([p_before_embeds, img_embeds, p_after_embeds], dim=1)\n",
    "        wrapped_atts_img = atts_img[:, :1].expand(-1, wrapped_img_embeds.shape[1])\n",
    "        return wrapped_img_embeds, wrapped_atts_img\n",
    "\n",
    "\n",
    "    def forward(self, samples):\n",
    "        image = samples[\"image\"]\n",
    "        question = samples[\"question\"]\n",
    "        img_embeds, atts_img = self.encode_img(image)\n",
    "        img_embeds = self.layer_norm(img_embeds)\n",
    "\n",
    "        img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img)\n",
    "\n",
    "        self.llama_tokenizer.padding_side = \"right\"\n",
    "        text = [t + self.end_sym for t in samples[\"answer\"]]\n",
    "\n",
    "        to_regress_tokens = self.llama_tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.hparams.max_length,\n",
    "            add_special_tokens=False\n",
    "        ).to(image[0].device)\n",
    "\n",
    "        targets = to_regress_tokens.input_ids.masked_fill(\n",
    "            to_regress_tokens.input_ids == 0, -100\n",
    "        )\n",
    "\n",
    "        empty_targets = (\n",
    "            torch.ones([atts_img.shape[0], atts_img.shape[1]+1],\n",
    "                       dtype=torch.long).to(image[0].device).fill_(-100)  # plus one for bos\n",
    "        )\n",
    "\n",
    "        targets = torch.cat([empty_targets, targets], dim=1)\n",
    "\n",
    "\n",
    "        batch_size = img_embeds.shape[0]\n",
    "        bos = torch.ones([batch_size, 1],\n",
    "                         dtype=to_regress_tokens.input_ids.dtype,\n",
    "                         device=to_regress_tokens.input_ids.device) * self.llama_tokenizer.bos_token_id\n",
    "        bos_embeds = self.embed_tokens(bos)\n",
    "        atts_bos = atts_img[:, :1]\n",
    "\n",
    "        to_regress_embeds = self.embed_tokens(to_regress_tokens.input_ids)\n",
    "        inputs_embeds = torch.cat([bos_embeds, img_embeds, to_regress_embeds], dim=1)\n",
    "        attention_mask = torch.cat([atts_bos, atts_img, to_regress_tokens.attention_mask], dim=1)\n",
    "\n",
    "        outputs = self.llama_model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "            labels=targets\n",
    "        )\n",
    "        \n",
    "        \n",
    "        loss = outputs.loss\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        result = self(batch)\n",
    "        self.log_dict(result, prog_bar=True)\n",
    "        return result\n",
    "\n",
    "    def save_checkpoint(self, eval_res):\n",
    "        current_epoch, global_step = self.trainer.current_epoch, self.trainer.global_step\n",
    "        param_grad_dic = {\n",
    "            k: v.requires_grad for (k, v) in self.named_parameters() if v.requires_grad\n",
    "        }\n",
    "        state_dict = self.state_dict()\n",
    "        for k in list(state_dict.keys()):\n",
    "            if k not in param_grad_dic.keys():\n",
    "                del state_dict[k]\n",
    "        save_obj = {\n",
    "            \"model\": state_dict,\n",
    "            \"config\": self.hparams,\n",
    "            \"epoch\": current_epoch,\n",
    "            \"step\":global_step\n",
    "        }\n",
    "        os.makedirs(os.path.join(self.hparams.savedmodel_path, 'checkpoints'), exist_ok=True)\n",
    "        save_to = os.path.join(\n",
    "            self.hparams.savedmodel_path, 'checkpoints',\n",
    "            \"checkpoint_epoch{}_step{}_bleu{:3f}_cider{:3f}.pth\".format(current_epoch, global_step, eval_res['Bleu_4'], eval_res['CIDEr']),\n",
    "        )\n",
    "        self.print(\"Saving checkpoint at step {} to {}.\".format(global_step, save_to))\n",
    "        torch.save(save_obj, save_to)\n",
    "    \n",
    "    def validation_step(self, samples, batch_idx):\n",
    "        self.llama_tokenizer.padding_side = \"right\"\n",
    "        to_regress_tokens = self.llama_tokenizer(\n",
    "            samples['answer'],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.hparams.max_length,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "\n",
    "        image = samples[\"image\"]\n",
    "        img_embeds, atts_img = self.encode_img(image)\n",
    "        img_embeds = self.layer_norm(img_embeds)\n",
    "        img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img)\n",
    "\n",
    "        batch_size = img_embeds.shape[0]\n",
    "        bos = torch.ones([batch_size, 1],\n",
    "                         dtype=atts_img.dtype,\n",
    "                         device=atts_img.device) * self.llama_tokenizer.bos_token_id\n",
    "        bos_embeds = self.embed_tokens(bos)\n",
    "        atts_bos = atts_img[:, :1]\n",
    "\n",
    "        inputs_embeds = torch.cat([bos_embeds, img_embeds], dim=1)\n",
    "        attention_mask = torch.cat([atts_bos, atts_img], dim=1)\n",
    "\n",
    "        outputs = self.llama_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            num_beams=self.hparams.beam_size,\n",
    "            do_sample=self.hparams.do_sample,\n",
    "            min_new_tokens=self.hparams.min_new_tokens,\n",
    "            max_new_tokens=self.hparams.max_new_tokens,\n",
    "            repetition_penalty=self.hparams.repetition_penalty,\n",
    "            length_penalty=self.hparams.length_penalty,\n",
    "            temperature=self.hparams.temperature,\n",
    "        )\n",
    "        hypo = [self.decode(i) for i in outputs]\n",
    "        ref = [self.decode(i) for i in to_regress_tokens['input_ids']]\n",
    "        self.val_step_outputs.append({\"hypo\": hypo, \"ref\": ref, \"id\": samples[\"image_id\"]})\n",
    "        return hypo, ref\n",
    "    \n",
    "    def decode(self, output_token):\n",
    "        if output_token[0] == 0:  # the model might output a unknow token <unk> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "        if output_token[0] == 1:  # some users find that there is a start token <s> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "        output_text = self.llama_tokenizer.decode(output_token, add_special_tokens=False)\n",
    "        output_text = output_text.split('</s>')[0].strip()\n",
    "        output_text = output_text.replace('<unk>', '')\n",
    "        return output_text\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        ref, hypo, ids = [], [], []\n",
    "        for i in self.val_step_outputs:\n",
    "            ref.extend(i['ref'])\n",
    "            hypo.extend(i['hypo'])\n",
    "            ids.extend(i['image_id'])\n",
    "\n",
    "        ref = {k:[v] for k, v in zip(ids, ref)}\n",
    "        hypo = {k:[v] for k, v in zip(ids, hypo)}\n",
    "        eval_res = self.score(ref=ref,hypo=hypo)\n",
    "        self.log_dict(eval_res, sync_dist=True, logger=True)\n",
    "\n",
    "        result_folder = os.path.join(self.hparams.savedmodel_path, 'result')\n",
    "        os.makedirs(result_folder, exist_ok=True)\n",
    "        current_epoch, global_step = self.trainer.current_epoch, self.trainer.global_step\n",
    "        json.dump(hypo, open(os.path.join(result_folder, f\"result_{current_epoch}_{global_step}\" + '.json'), 'w'))\n",
    "        json.dump(ref, open(os.path.join(result_folder, 'refs.json'), 'w'))\n",
    "        self.print(eval_res)\n",
    "\n",
    "        val_score = 0\n",
    "        for score_type, weight in zip(self.hparams.scorer_types, self.hparams.weights):\n",
    "            val_score += eval_res[score_type] * weight\n",
    "\n",
    "        if self.trainer.local_rank == 0:\n",
    "            if val_score > self.val_score:\n",
    "                self.save_checkpoint(eval_res)\n",
    "                self.val_score = val_score\n",
    "        self.val_step_outputs.clear()\n",
    "\n",
    "\n",
    "    def test_step(self, samples, batch_idx):\n",
    "        self.llama_tokenizer.padding_side = \"right\"\n",
    "        to_regress_tokens = self.llama_tokenizer(\n",
    "            samples['input_text'],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.hparams.max_length,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "\n",
    "        image = samples[\"image\"]\n",
    "        img_embeds, atts_img = self.encode_img(image)\n",
    "        img_embeds = self.layer_norm(img_embeds)\n",
    "        img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img)\n",
    "\n",
    "        batch_size = img_embeds.shape[0]\n",
    "        bos = torch.ones([batch_size, 1],\n",
    "                         dtype=atts_img.dtype,\n",
    "                         device=atts_img.device) * self.llama_tokenizer.bos_token_id\n",
    "        bos_embeds = self.embed_tokens(bos)\n",
    "        atts_bos = atts_img[:, :1]\n",
    "\n",
    "        inputs_embeds = torch.cat([bos_embeds, img_embeds], dim=1)\n",
    "        attention_mask = torch.cat([atts_bos, atts_img], dim=1)\n",
    "\n",
    "        outputs = self.llama_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            num_beams=self.hparams.beam_size,\n",
    "            do_sample=self.hparams.do_sample,\n",
    "            min_new_tokens=self.hparams.min_new_tokens,\n",
    "            max_new_tokens=self.hparams.max_new_tokens,\n",
    "            repetition_penalty=self.hparams.repetition_penalty,\n",
    "            length_penalty=self.hparams.length_penalty,\n",
    "            temperature=self.hparams.temperature,\n",
    "        )\n",
    "        hypo = [self.decode(i) for i in outputs]\n",
    "        ref = [self.decode(i) for i in to_regress_tokens['input_ids']]\n",
    "        self.test_step_outputs.append({\"hypo\": hypo, \"ref\": ref, \"id\": samples[\"image_id\"]})\n",
    "        return hypo, ref\n",
    "\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"\n",
    "        This function is called at the end of the test epoch.\n",
    "        It is recommended to test on single device to ensure each sample/batch gets evaluated exactly once. This is helpful to make sure benchmarking for research papers is done the right way. Otherwise, in a multi-device setting, samples could occur duplicated when DistributedSampler is used, for eg. with strategy=\"ddp\". It replicates some samples on some devices to make sure all devices have same batch size in case of uneven inputs.\n",
    "        \"\"\"\n",
    "        ref, hypo, ids = [], [], []\n",
    "        for i in self.test_step_outputs:\n",
    "            ref.extend(i['ref'])\n",
    "            hypo.extend(i['hypo'])\n",
    "            ids.extend(i['image_id'])\n",
    "\n",
    "        ref = {k:[v] for k, v in zip(ids, ref)}\n",
    "        hypo = {k:[v] for k, v in zip(ids, hypo)}\n",
    "        eval_res = self.score(ref=ref,hypo=hypo)\n",
    "\n",
    "        result_folder = os.path.join(self.hparams.savedmodel_path, 'result')\n",
    "        os.makedirs(result_folder, exist_ok=True)\n",
    "        json.dump(hypo, open(os.path.join(result_folder, f\"test_result.json\"), 'w'))\n",
    "        json.dump(ref, open(os.path.join(result_folder, 'test_refs.json'), 'w'))\n",
    "        self.print(f\"Test result of {self.hparams.delta_file}: {eval_res}\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=self.hparams.max_epochs, eta_min=1e-6)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "    def get_progress_bar_dict(self):\n",
    "        # don't show the version number\n",
    "        items = super().get_progress_bar_dict()\n",
    "        items.pop(\"v_num\", None)\n",
    "        return items\n",
    "\n",
    "    def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
